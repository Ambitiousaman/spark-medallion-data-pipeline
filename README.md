# ğŸš€ Spark Medallion Data Pipeline

An end-to-end **Data Engineering pipeline** built using **PySpark, Docker, and Medallion Architecture** to simulate a production-grade distributed data processing workflow.

This project demonstrates schema enforcement, distributed processing, layered data architecture, and analytics integration.

---

## ğŸ— Architecture Overview

The pipeline follows the **Medallion Architecture** pattern:

Raw (CSV) â†’ Bronze (Schema Enforced) â†’ Silver (Cleaned & Validated) â†’ Gold (Aggregated Business Layer)

---

## âš™ï¸ Tech Stack

- ğŸ Python â€“ Synthetic Data Generation  
- âš¡ PySpark â€“ Distributed Data Processing  
- ğŸ³ Docker â€“ Spark Cluster Deployment  
- ğŸ§± Medallion Architecture â€“ Layered Data Design  
- ğŸ“¦ Parquet â€“ Optimized Columnar Storage  
- ğŸ“Š Power BI â€“ Analytics & Visualization  

---

## ğŸ–¥ Distributed Spark Cluster

The pipeline runs on a Dockerized Spark cluster:

- 1 Spark Master  
- 2 Spark Workers  
- Shared volume for data storage  
- Spark UI enabled for monitoring  

This setup simulates a real distributed data processing environment.

---

## ğŸ“‚ Data Flow

### 1ï¸âƒ£ Raw Layer
- Synthetic retail dataset generated using Python  
- Stored as CSV  
- No transformation applied  

### 2ï¸âƒ£ Bronze Layer
- Schema enforced during ingestion  
- Converted from CSV â†’ Parquet  
- Structured and standardized  

### 3ï¸âƒ£ Silver Layer
Data quality validations applied:
- Removed duplicate transaction IDs  
- Filtered invalid quantities and prices  
- Validated discount ranges  
- Standardized categorical values  
- Fixed date inconsistencies  

Result: Clean, analytics-ready dataset.

### 4ï¸âƒ£ Gold Layer
- Business aggregations (Revenue, KPIs, City-level metrics)  
- Optimized for BI consumption  

---

## â–¶ï¸ How to Run the Project

### 1ï¸âƒ£ Start Spark Cluster
```bash
docker-compose up -d
2ï¸âƒ£ Generate Raw Data
python generate_raw_data.py

3ï¸âƒ£ Run Spark Job
docker exec -it spark-master spark-submit /app/etl_pipeline.py

4ï¸âƒ£ Monitor Spark UI
Open in your browser:
http://localhost:8080

ğŸ“Š Analytics Layer
The Gold dataset is connected to Power BI for:
Revenue Analysis
City-Level Sales Mapping
KPI Dashboarding

ğŸ”¥ Key Data Engineering Concepts Implemented

Schema enforcement during ingestion
Distributed transformations with Spark
Parquet optimization over CSV
Data validation & quality rules
Containerized big data environment
Layered storage architecture

ğŸ¯ Learning Outcomes

This project strengthened my understanding of:
Distributed Spark processing
Storage optimization techniques
Real-world data pipeline architecture
Docker-based cluster deployment
Medallion data modeling principles

ğŸš€ Future Improvements

Incremental data loads
Partitioning strategies
Airflow orchestration
CI/CD integration
Cloud deployment (AWS / Azure)
